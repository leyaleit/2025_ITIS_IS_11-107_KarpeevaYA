import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import os
import sys
import time

def main(start_urls):
    global to_visit
    os.makedirs(base_output_dir, exist_ok=True)

    if not start_urls:
        print("Ошибка: Не указаны стартовые URL")
        return

    print(f"Начало сканирования с {len(start_urls)} стартовых URL")
    while to_visit and doc_count < max_docs:
        url = to_visit.pop(0)
        crawl(url)
        time.sleep(1)

    print(f"Готово! Скачано {doc_count} страниц.") 