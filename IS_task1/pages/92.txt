Нейронная сеть — Википедия Нейронная сеть Материал из Википедии — свободной энциклопедии (перенаправлено с « Искусственная нейронная сеть ») Текущая версия страницы пока не проверялась опытными участниками и может значительно отличаться от версии, проверенной 13 марта 2025 года ; проверки требуют 2 правки . Перейти к навигации Перейти к поиску Эта статья о понятии в программировании; о сетях нервных клеток живых организмов см. Нервная сеть . Схема простой нейросети. Зелёным цветом обозначены входные нейроны, голубым — скрытые нейроны, жёлтым — выходной нейрон Нейро́нная сеть [ 1 ] (также иску́сственная нейро́нная сеть , ИНС , или просто нейросе́ть ) — математическая модель , а также её программное или аппаратное воплощение, построенная по принципу организации биологических нейронных сетей — сетей нервных клеток живого организма. Это понятие возникло при изучении процессов, протекающих в мозге , и при попытке смоделировать эти процессы. Первой такой попыткой были нейронные сети У. Маккалока и У. Питтса [ 2 ] . После разработки алгоритмов обучения получаемые модели стали использовать в практических целях: в задачах прогнозирования , для распознавания образов , в задачах управления и др. ИНС представляет собой систему соединённых и взаимодействующих между собой простых процессоров ( искусственных нейронов ). Такие процессоры довольно просты (особенно в сравнении с процессорами, используемыми в персональных компьютерах). Каждый процессор подобной сети имеет дело только с сигналами , которые он периодически получает, и сигналами, которые он периодически посылает другим процессорам. И, тем не менее, будучи соединёнными в достаточно большую сеть с управляемым взаимодействием, такие простые по отдельности процессоры вместе способны выполнять довольно сложные задачи. С точки зрения машинного обучения , нейронная сеть представляет собой частный случай методов распознавания образов , дискриминантного анализа ; С точки зрения математики , обучение нейронных сетей — это многопараметрическая задача нелинейной оптимизации ; С точки зрения кибернетики , нейронная сеть используется в задачах адаптивного управления и как алгоритмы для робототехники ; С точки зрения развития вычислительной техники и программирования , нейронная сеть — способ решения проблемы эффективного параллелизма [ 3 ] ; С точки зрения искусственного интеллекта , ИНС является основой философского течения коннекционизма и основным направлением в структурном подходе по изучению возможности построения (моделирования) естественного интеллекта с помощью компьютерных алгоритмов . Нейронные сети не программируются в привычном смысле этого слова, они обучаются [ a ] . Возможность обучения — одно из главных преимуществ нейронных сетей перед традиционными алгоритмами . Технически обучение заключается в нахождении коэффициентов связей между нейронами. В процессе обучения нейронная сеть способна выявлять сложные зависимости между входными и выходными данными, а также выполнять обобщение . Это значит, что в случае успешного обучения сеть сможет вернуть верный результат на основании данных, которые отсутствовали в обучающей выборке, а также неполных и/или «зашумлённых», частично искажённых данных. Содержание 1 Хронология 2 Известные применения 2.1 Распознавание образов и классификация 2.1.1 Используемые архитектуры нейросетей 2.2 Принятие решений и управление 2.3 Кластеризация 2.3.1 Используемые архитектуры нейросетей 2.4 Прогнозирование 2.4.1 Используемые архитектуры нейросетей 2.5 Аппроксимация 2.5.1 Используемые архитектуры нейросетей 2.6 Сжатие данных и ассоциативная память 2.7 Анализ данных 2.7.1 Используемые архитектуры нейросетей 2.8 Оптимизация 2.8.1 Используемые архитектуры нейросетей 3 Этапы решения задач 3.1 Сбор данных для обучения 3.2 Выбор топологии сети 3.3 Экспериментальный подбор характеристик сети 3.4 Экспериментальный подбор параметров обучения 3.5 Обучение сети 3.6 Проверка адекватности обучения 4 Классификация по типу входной информации 5 Классификация по характеру обучения 6 Классификация по характеру настройки синапсов 7 Классификация по времени передачи сигнала 8 Классификация по характеру связей 8.1 Нейронные сети прямого распространения 8.2 Рекуррентные нейронные сети 8.3 Радиально-базисные функции 8.4 Самоорганизующиеся карты 9 Известные типы сетей 10 Отличия от машин с архитектурой фон Неймана 11 Примеры использований 11.1 Предсказание финансовых временных рядов 11.2 Психодиагностика 11.3 Хемоинформатика 11.4 Нейроуправление 11.5 Экономика 11.6 Самостоятельные системы 11.7 Системы рекомендаций 11.8 Нейросети в астрономии 12 См. также 13 Примечания 13.1 Комментарии 13.2 Сноски 14 Литература 15 Ссылки Хронология [ править | править код ] 1943 — У. Маккалок и У. Питтс формализуют понятие нейронной сети в фундаментальной статье о логическом исчислении идей и нервной активности [ 2 ] . В начале своего сотрудничества с Питтсом Н. Винер предлагает ему вакуумные лампы в качестве средства для реализации эквивалентов нейронных сетей [ 5 ] . 1948 — опубликована книга Н. Винера о кибернетике. Основной идеей стало представление сложных биологических процессов математическими моделями. 1949 — Д. Хебб предлагает первый алгоритм обучения. 1958 — Ф. Розенблатт изобретает однослойный перцептрон и демонстрирует его способность решать задачи классификации [ 6 ] . Перцептрон использовали для распознавания образов, прогнозирования погоды. К моменту изобретения перцептрона завершилось расхождение теоретических работ Маккалока с «кибернетикой» Винера; Маккалок и его последователи вышли из состава «Кибернетического клуба». 1960 — Бернард Уидроу [англ.] совместно со своим студентом Хоффом на основе дельта-правила ( формулы Уидроу ) разработали Адалин, который сразу начал использоваться для задач предсказания и адаптивного управления. Адалин был построен на базе созданных ими же (Уидроу — Хоффом) новых элементах — мемисторах [ 7 ] [ 8 ] . 1963 — в Институте проблем передачи информации АН СССР А. П. Петровым проводится исследование задач, «трудных» для перцептрона [ 9 ] . На эту работу в области моделирования ИНС в СССР опирался М. М. Бонгард в своей работе как «сравнительно небольшой переделкой алгоритма (перцептрона) исправить его недостатки» [ 10 ] . 1969 — М. Минский публикует формальное доказательство ограниченности перцептрона и показывает, что он не способен решать некоторые задачи (проблема «чётности» и «один в блоке»), связанные с инвариантностью представлений. 1972 — Т. Кохонен и Дж. Андерсон [англ.] независимо предлагают новый тип нейронных сетей, способных функционировать в качестве памяти [ 11 ] . 1973 — Б. В. Хакимов предлагает нелинейную модель с синапсами на основе сплайнов и внедряет её для решения задач в медицине, геологии, экологии [ 12 ] . 1974 — Пол Дж. Вербос [ 13 ] и Галушкин А. И. [ 14 ] одновременно изобретают алгоритм обратного распространения ошибки для обучения многослойных перцептронов [ 15 ] . 1975 — Фукусима [англ.] представляет когнитрон — самоорганизующуюся сеть, предназначенную для инвариантного распознавания образов , но это достигается только при помощи запоминания практически всех состояний образа. 1982 — Дж. Хопфилд показал, что нейронная сеть с обратными связями может представлять собой систему, минимизирующую энергию ( сеть Хопфилда ). Кохоненом представлены модели сети, обучающейся без учителя ( нейронная сеть Кохонена ), решающей задачи кластеризации , визуализации данных ( самоорганизующаяся карта Кохонена ) и другие задачи предварительного анализа данных. 1986 — Дэвидом И. Румельхартом , Дж. Е. Хинтоном и Рональдом Дж. Вильямсом [ 16 ] , а также независимо и одновременно С. И. Барцевым и В. А. Охониным [ 17 ] , переоткрыт и развит метод обратного распространения ошибки . 2007 — Джеффри Хинтоном в университете Торонто созданы алгоритмы глубокого обучения многослойных нейронных сетей. Хинтон при обучении нижних слоёв сети использовал ограниченную машину Больцмана (RBM — Restricted Boltzmann Machine). По Хинтону необходимо использовать много примеров распознаваемых образов (например, множество лиц людей на разных фонах). После обучения получается готовое быстро работающее приложение, способное решать конкретную задачу (например, осуществлять поиск лиц на изображении). Известные применения [ править | править код ] Распознавание образов и классификация [ править | править код ] Основные статьи: Теория распознавания образов и Задача классификации В качестве образов могут выступать различные по своей природе объекты: символы текста, изображения, образцы звуков и так далее. При обучении сети предлагаются различные образцы образов с указанием того, к какому классу они относятся. Образец, как правило, представляется как вектор значений признаков. При этом совокупность всех признаков должна однозначно определять класс , к которому относится образец. В случае, если признаков недостаточно, сеть может соотнести один и тот же образец с несколькими классами, что неверно. По окончании обучения сети ей можно предъявлять неизвестные ранее образы и получать ответ о принадлежности к определённому классу. Топология такой сети характеризуется тем, что количество нейронов в выходном слое, как правило, равно количеству определяемых классов. При этом устанавливается соответствие между выходом нейронной сети и классом, который он представляет. Когда сети предъявляется некий образ, на одном из её выходов должен появиться признак того, что образ принадлежит этому классу. В то же время на других выходах должен быть признак того, что образ данному классу не принадлежит [ 18 ] . Если на двух или более выходах есть признак принадлежности к классу, считается, что сеть «не уверена» в своём ответе. Используемые архитектуры нейросетей [ править | править код ] Обучение с учителем: Перцептрон Свёрточные нейронные сети Обучение без учителя: Сети адаптивного резонанса Смешанное обучение: Сеть радиально-базисных функций Принятие решений и управление [ править | править код ] Эта задача близка к задаче классификации. Классификации подлежат ситуации, характеристики которых поступают на вход нейронной сети. На выходе сети при этом должен появиться признак решения, которое она приняла. При этом в качестве входных сигналов используются различные критерии описания состояния управляемой системы [ 19 ] . Кластеризация [ править | править код ] Основная статья: Кластерный анализ В разделе не хватает ссылок на источники (см. рекомендации по поиску ). Информация должна быть проверяема , иначе она может быть удалена. Вы можете отредактировать статью, добавив ссылки на авторитетные источники в виде сносок . ( 7 декабря 2023 ) Под кластеризацией понимается разбиение множества входных сигналов на классы, при том, что ни количество, ни признаки классов заранее неизвестны. После обучения такая сеть способна определять, к какому классу относится входной сигнал. Сеть также может сигнализировать о том, что входной сигнал не относится ни к одному из выделенных классов — это является признаком новых, отсутствующих в обучающей выборке, данных. Таким образом, подобная сеть может выявлять новые, неизвестные ранее классы сигналов . Соответствие между классами, выделенными сетью, и классами, существующими в предметной области, устанавливается человеком. Кластеризацию осуществляют, например, нейронные сети Кохонена . Нейронные сети в простом варианте Кохонена не могут быть огромными, поэтому их делят на гиперслои (гиперколонки) и ядра (микроколонки). Если сравнивать с мозгом человека, то идеальное количество параллельных слоёв не должно быть более 112. Эти слои в свою очередь составляют гиперслои (гиперколонку), в которой от 500 до 2000 микроколонок (ядер). При этом каждый слой делится на множество гиперколонок, пронизывающих насквозь эти слои. Микроколонки кодируются цифрами и единицами с получением результата на выходе. Если требуется, то лишние слои и нейроны удаляются или добавляются. Идеально для подбора числа нейронов и слоёв использовать суперкомпьютер. Такая система позволяет нейронным сетям быть пластичными. Используемые архитектуры нейросетей [ править | править код ] Обучение без учителя: Перцептрон Самоорганизующаяся карта Кохонена Нейронная сеть Кохонена Сети адаптивного резонанса Прогнозирование [ править | править код ] Основная статья: Прогнозирование В разделе не хватает ссылок на источники (см. рекомендации по поиску ). Информация должна быть проверяема , иначе она может быть удалена. Вы можете отредактировать статью, добавив ссылки на авторитетные источники в виде сносок . ( 7 декабря 2023 ) Способности нейронной сети к прогнозированию напрямую следуют из её способности к обобщению и выделению скрытых зависимостей между входными и выходными данными. После обучения сеть способна предсказать будущее значение некой последовательности на основе нескольких предыдущих значений и (или) каких-то существующих в настоящий момент факторов. Прогнозирование возможно только тогда, когда предыдущие изменения в какой-то степени действительно предопределяют будущие . Например, прогнозирование котировок акций на основе котировок за прошлую неделю может оказаться успешным (а может и не оказаться), тогда как прогнозирование результатов завтрашней лотереи на основе данных за последние 50 лет почти наверняка не даст никаких результатов. Используемые архитектуры нейросетей [ править | править код ] Обучение с учителем: Перцептрон Смешанное обучение: Сеть радиально-базисных функций Аппроксимация [ править | править код ] Основная статья: Аппроксимация В разделе не хватает ссылок на источники (см. рекомендации по поиску ). Информация должна быть проверяема , иначе она может быть удалена. Вы можете отредактировать статью, добавив ссылки на авторитетные источники в виде сносок . ( 7 декабря 2023 ) Нейронные сети могут аппроксимировать непрерывные функции. Доказана обобщённая аппроксимационная теорема [ 20 ] : с помощью линейных операций и каскадного соединения можно из произвольного нелинейного элемента получить устройство, вычисляющее любую непрерывную функцию с некоторой наперёд заданной точностью . Это означает, что нелинейная характеристика нейрона может быть произвольной: от сигмоидальной до произвольного волнового пакета или вейвлета , синуса или многочлена . От выбора нелинейной функции может зависеть сложность конкретной сети, но с любой нелинейностью сеть остаётся универсальным аппроксиматором и при правильном выборе структуры может достаточно точно аппроксимировать функционирование любого непрерывного автомата. Используемые архитектуры нейросетей [ править | править код ] Обучение с учителем: Перцептрон Смешанное обучение: Сеть радиально-базисных функций Сжатие данных и ассоциативная память [ править | править код ] Основные статьи: Нейросетевое сжатие данных и Ассоциативная память Способность нейросетей к выявлению взаимосвязей между различными параметрами даёт возможность выразить данные большой размерности более компактно, если данные тесно взаимосвязаны друг с другом. Обратный процесс — восстановление исходного набора данных из части информации — называется (авто)ассоциативной памятью. Ассоциативная память позволяет также восстанавливать исходный сигнал/образ из зашумлённых/повреждённых входных данных. Решение задачи гетероассоциативной памяти позволяет реализовать память, адресуемую по содержимому [ 19 ] . Анализ данных [ править | править код ] Используемые архитектуры нейросетей [ править | править код ] Обучение с учителем: Перцептрон Обучение без учителя: Перцептрон Самоорганизующаяся карта Кохонена Нейронная сеть Кохонена Оптимизация [ править | править код ] Используемые архитектуры нейросетей [ править | править код ] Обучение без учителя: Самоорганизующаяся карта Кохонена Нейронная сеть Кохонена Этапы решения задач [ править | править код ] Сбор данных для обучения; Подготовка и нормализация данных; Выбор топологии сети; Экспериментальный подбор характеристик сети; Экспериментальный подбор параметров обучения; Собственно обучение; Проверка адекватности обучения; Корректировка параметров, окончательное обучение; Вербализация сети [ 21 ] с целью дальнейшего использования. Следует рассмотреть подробнее некоторые из этих этапов. Сбор данных для обучения [ править | править код ] В разделе не хватает ссылок на источники (см. рекомендации по поиску ). Информация должна быть проверяема , иначе она может быть удалена. Вы можете отредактировать статью, добавив ссылки на авторитетные источники в виде сносок . ( 3 ноября 2023 ) Выбор данных для обучения сети и их обработка является самым сложным этапом решения задачи. Набор данных для обучения должен удовлетворять нескольким критериям: репрезентативность — данные должны иллюстрировать истинное положение вещей в предметной области; непротиворечивость — противоречивые данные в обучающей выборке приведут к плохому качеству обучения сети. Исходные данные преобразуются к виду, в котором их можно подать на входы сети. Каждая запись в файле данных называется обучающей парой или обучающим вектором . Обучающий вектор содержит по одному значению на каждый вход сети и, в зависимости от типа обучения (с учителем или без), по одному значению для каждого выхода сети. Обучение сети на «сыром» наборе, как правило, не даёт качественных результатов. Существует ряд способов улучшить «восприятие» сети. Нормировка выполняется, когда на различные входы подаются данные разной размерности. Например, на первый вход сети подаются величины со значениями от нуля до единицы, а на второй — от ста до тысячи. При отсутствии нормировки значения на втором входе будут всегда оказывать существенно большее влияние на выход сети, чем значения на первом входе. При нормировке размерности всех входных и выходных данных сводятся воедино; квантование выполняется над непрерывными величинами, для которых выделяется конечный набор дискретных значений. Например, квантование используют для задания частот звуковых сигналов при распознавании речи; фильтрация выполняется для «зашумлённых» данных. Кроме того, большую роль играет само представление как входных, так и выходных данных. Предположим, сеть обучается распознаванию букв на изображениях и имеет один числовой выход — номер буквы в алфавите. В этом случае сеть получит ложное представление о том, что буквы с номерами 1 и 2 более похожи, чем буквы с номерами 1 и 3, что, в общем, неверно. Для того, чтобы избежать такой ситуации, используют топологию сети с большим числом выходов, когда каждый выход имеет свой смысл. Чем больше выходов в сети, тем большее расстояние между классами и тем сложнее их спутать. Выбор топологии сети [ править | править код ] В разделе не хватает ссылок на источники (см. рекомендации по поиску ). Информация должна быть проверяема , иначе она может быть удалена. Вы можете отредактировать статью, добавив ссылки на авторитетные источники в виде сносок . ( 3 ноября 2023 ) Выбирать тип сети следует, исходя из постановки задачи и имеющихся данных для обучения. Для обучения с учителем требуется наличие для каждого элемента выборки «экспертной» оценки. Иногда получение такой оценки для большого массива данных просто невозможно. В этих случаях естественным выбором является сеть, обучающаяся без учителя (например, самоорганизующаяся карта Кохонена или нейронная сеть Хопфилда ). При решении других задач (таких, как прогнозирование временных рядов) экспертная оценка уже содержится в исходных данных и может быть выделена при их обработке. В этом случае можно использовать многослойный перцептрон [ уточнить ] или сеть Ворда . Экспериментальный подбор характеристик сети [ править | править код ] В разделе не хватает ссылок на источники (см. рекомендации по поиску ). Информация должна быть проверяема , иначе она может быть удалена. Вы можете отредактировать статью, добавив ссылки на авторитетные источники в виде сносок . ( 3 ноября 2023 ) После выбора общей структуры нужно экспериментально подобрать параметры сети. Для сетей, подобных перцептрону, это будет число слоёв, число блоков в скрытых слоях (для сетей Ворда), наличие или отсутствие обходных соединений, передаточные функции нейронов. При выборе количества слоёв и нейронов в них следует исходить из того, что способности сети к обобщению тем выше, чем больше суммарное число связей между нейронами . С другой стороны, число связей ограничено сверху количеством записей в обучающих данных. Экспериментальный подбор параметров обучения [ править | править код ] В разделе не хватает ссылок на источники (см. рекомендации по поиску ). Информация должна быть проверяема , иначе она может быть удалена. Вы можете отредактировать статью, добавив ссылки на авторитетные источники в виде сносок . ( 3 ноября 2023 ) После выбора конкретной топологии необходимо выбрать параметры обучения нейронной сети. Этот этап особенно важен для сетей, обучающихся с учителем . От правильного выбора параметров зависит не только то, насколько быстро ответы сети будут сходиться к правильным ответам. Например, выбор низкой скорости обучения увеличит время схождения, однако иногда позволяет избежать паралича сети . Увеличение момента обучения может привести как к увеличению, так и к уменьшению времени сходимости, в зависимости от формы поверхности ошибки . Исходя из такого противоречивого влияния параметров, можно сделать вывод, что их значения нужно выбирать экспериментально, руководствуясь при этом критерием завершения обучения (например, минимизация ошибки или ограничение по времени обучения). Обучение сети [ править | править код ] В разделе не хватает ссылок на источники (см. рекомендации по поиску ). Информация должна быть проверяема , иначе она может быть удалена. Вы можете отредактировать статью, добавив ссылки на авторитетные источники в виде сносок . ( 3 ноября 2023 ) В процессе обучения сеть в определённом порядке просматривает обучающую выборку. Порядок просмотра может быть последовательным, случайным и т. д. Некоторые сети, обучающиеся без учителя (например, сети Хопфилда ), просматривают выборку только один раз. Другие (например, сети Кохонена ), а также сети, обучающиеся с учителем, просматривают выборку множество раз, при этом один полный проход по выборке называется эпохой обучения . При обучении с учителем набор исходных данных делят на две части — собственно обучающую выборку и тестовые данные; принцип разделения может быть произвольным. Обучающие данные подаются сети для обучения, а проверочные используются для расчёта ошибки сети (проверочные данные никогда для обучения сети не применяются). Таким образом, если на проверочных данных ошибка уменьшается, то сеть действительно выполняет обобщение. Если ошибка на обучающих данных продолжает уменьшаться, а ошибка на тестовых данных увеличивается, значит, сеть перестала выполнять обобщение и просто «запоминает» обучающие данные. Это явление называется переобучением сети или оверфиттингом . В таких случаях обучение обычно прекращают. В процессе обучения могут проявиться другие проблемы, такие как паралич или попадание сети в локальный минимум поверхности ошибок. Невозможно заранее предсказать проявление той или иной проблемы, равно как и дать однозначные рекомендации к их разрешению. Всё выше сказанное относится только к итерационным алгоритмам поиска нейросетевых решений. Для них действительно нельзя ничего гарантировать и нельзя полностью автоматизировать обучение нейронных сетей. [ источник не указан 4314 дней ] Однако, наряду с итерационными алгоритмами обучения, существуют не итерационные алгоритмы, обладающие очень высокой устойчивостью и позволяющие полностью автоматизировать процесс обучения [ источник не указан 2688 дней ] . Проверка адекватности обучения [ править | править код ] Даже в случае успешного, на первый взгляд, обучения сеть не всегда обучается именно тому, чего от неё хотел создатель. Известен случай, когда сеть обучалась распознаванию изображений танков по фотографиям, однако позднее выяснилось, что все танки были сфотографированы на одном и том же фоне. В результате сеть «научилась» распознавать этот тип ландшафта, вместо того, чтобы «научиться» распознавать танки [ 22 ] . Таким образом, сеть «понимает» не то, что от неё требовалось, а то, что проще всего обобщить. Тестирование качества обучения нейросети необходимо проводить на примерах, которые не участвовали в её обучении. При этом число тестовых примеров должно быть тем больше, чем выше качество обучения. Если ошибки нейронной сети имеют вероятность близкую к одной миллиардной, то и для подтверждения этой вероятности нужен миллиард тестовых примеров. Получается, что тестирование хорошо обученных нейронных сетей становится очень трудной задачей. Классификация по типу входной информации [ править | править код ] В разделе не хватает ссылок на источники (см. рекомендации по поиску ). Информация должна быть проверяема , иначе она может быть удалена. Вы можете отредактировать статью, добавив ссылки на авторитетные источники в виде сносок . ( 7 декабря 2023 ) Аналоговые нейронные сети (используют информацию в форме действительных чисел); Двоичные нейронные сети (оперируют с информацией, представленной в двоичном виде); Образные нейронные сети (оперируют с информацией, представленной в виде образов: знаков, иероглифов, символов). Классификация по характеру обучения [ править | править код ] В разделе не хватает ссылок на источники (см. рекомендации по поиску ). Информация должна быть проверяема , иначе она может быть удалена. Вы можете отредактировать статью, добавив ссылки на авторитетные источники в виде сносок . ( 7 декабря 2023 ) Обучение с учителем — выходное пространство решений нейронной сети известно; Обучение без учителя — нейронная сеть формирует выходное пространство решений только на основе входных воздействий. Такие сети называют самоорганизующимися; Обучение с подкреплением — система назначения штрафов и поощрений от среды. Классификация по характеру настройки синапсов [ править | править код ] В разделе не хватает ссылок на источники (см. рекомендации по поиску ). Информация должна быть проверяема , иначе она может быть удалена. Вы можете отредактировать статью, добавив ссылки на авторитетные источники в виде сносок . ( 7 декабря 2023 ) Сети с фиксированными связями (весовые коэффициенты нейронной сети выбираются сразу, исходя из условий задачи, при этом: d W / d t = 0 {\displaystyle {\boldsymbol {d}}W/dt=0} , где W — весовые коэффициенты сети); Сети с динамическими связями (для них в процессе обучения происходит настройка синаптических связей, то есть d W / d t ≠ 0 {\displaystyle {\boldsymbol {d}}W/dt\not =0} , где W — весовые коэффициенты сети). Классификация по времени передачи сигнала [ править | править код ] В разделе не хватает ссылок на источники (см. рекомендации по поиску ). Информация должна быть проверяема , иначе она может быть удалена. Вы можете отредактировать статью, добавив ссылки на авторитетные источники в виде сносок . ( 7 декабря 2023 ) В ряде нейронных сетей активирующая функция может зависеть не только от весовых коэффициентов связей w i j {\displaystyle w_{ij}} , но и от времени передачи импульса (сигнала) по каналам связи τ i j {\displaystyle \tau _{ij}} . Поэтому в общем виде активирующая (передающая) функция связи c i j {\displaystyle c_{ij}} от элемента u i {\displaystyle u_{i}} к элементу u j {\displaystyle u_{j}} имеет вид: c i j ∗ = f [ w i j ( t ) , u i ∗ ( t − τ i j ) ] {\displaystyle c_{ij}^{*}=f[w_{ij}(t),u_{i}^{*}(t-\tau _{ij})]} .
Тогда синхронной сетью называют такую сеть, у которой время передачи τ i j {\displaystyle \tau _{ij}} каждой связи равно либо нулю, либо фиксированной постоянной τ {\displaystyle \tau } . Асинхронной называют такую сеть, у которой время передачи τ i j {\displaystyle \tau _{ij}} для каждой связи между элементами u i {\displaystyle u_{i}} и u j {\displaystyle u_{j}} своё, но тоже постоянное. Классификация по характеру связей [ править | править код ] Нейронные сети прямого распространения [ править | править код ] В разделе не хватает ссылок на источники (см. рекомендации по поиску ). Информация должна быть проверяема , иначе она может быть удалена. Вы можете отредактировать статью, добавив ссылки на авторитетные источники в виде сносок . ( 7 декабря 2023 ) В нейронных сетях прямого распространения ( англ. feedforward neural network ) все связи направлены строго от входных нейронов к выходным. Примерами таких сетей являются перцептрон Розенблатта , многослойный перцептрон , сети Ворда . Рекуррентные нейронные сети [ править | править код ] Основная статья: Рекуррентная нейронная сеть Сигнал с выходных нейронов или нейронов скрытого слоя частично передаётся обратно на входы нейронов входного слоя ( обратная связь ). Рекуррентная сеть Хопфилда «фильтрует» входные данные, возвращаясь к устойчивому состоянию и, таким образом, позволяет решать задачи компрессии данных и построения ассоциативной памяти [ 23 ] . Частным случаем рекуррентных сетей являются двунаправленные сети. В таких сетях между слоями существуют связи как в направлении от входного слоя к выходному, так и в обратном. Классическим примером является Нейронная сеть Коско . Радиально-базисные функции [ править | править код ] Основная статья: Сеть радиально-базисных функций В разделе не хватает ссылок на источники (см. рекомендации по поиску ). Информация должна быть проверяема , иначе она может быть удалена. Вы можете отредактировать статью, добавив ссылки на авторитетные источники в виде сносок . ( 7 декабря 2023 ) Разработаны нейронные сети, использующие в качестве активационных функций радиально-базисные (также называются RBF-сетями). Общий вид радиально-базисной функции: f ( x ) = ϕ ( x 2 σ 2 ) {\displaystyle f(x)=\phi \left({\frac {x^{2}}{\sigma ^{2}}}\right)} , например, f ( x ) = e − x 2 σ 2 , {\displaystyle f(x)=e^{-{{x^{2}} \over {\sigma ^{2}}}},} где x {\displaystyle x} — вектор входных сигналов нейрона, σ {\displaystyle \sigma } — ширина окна функции, ϕ ( y ) {\displaystyle \phi (y)} — убывающая функция (чаще всего, равная нулю вне некоторого отрезка). Радиально-базисная сеть характеризуется тремя особенностями: единственный скрытый слой; только нейроны скрытого слоя имеют нелинейную активационную функцию; синаптические веса связей входного и скрытого слоёв равны единице. Самоорганизующиеся карты [ править | править код ] Основная статья: Самоорганизующаяся карта Кохонена Такие сети представляют собой соревновательную нейронную сеть с обучением без учителя , выполняющую задачу визуализации и кластеризации . Является методом проецирования многомерного пространства в пространство с более низкой размерностью (чаще всего, двумерное), применяется также для решения задач моделирования, прогнозирования и др. Является одной из версий нейронных сетей Кохонена [ 24 ] . Самоорганизующиеся карты Кохонена служат, в первую очередь, для визуализации и первоначального («разведывательного») анализа данных [ 25 ] . Сигнал в сеть Кохонена поступает сразу на все нейроны, веса соответствующих синапсов интерпретируются как координаты положения узла, и выходной сигнал формируется по принципу «победитель забирает всё» — то есть ненулевой выходной сигнал имеет нейрон, ближайший (в смысле весов синапсов) к подаваемому на вход объекту. В процессе обучения веса синапсов настраиваются таким образом, чтобы узлы решётки «располагались» в местах локальных сгущений данных, то есть описывали кластерную структуру облака данных, с другой стороны, связи между нейронами соответствуют отношениям соседства между соответствующими кластерами в пространстве признаков. Удобно рассматривать такие карты как двумерные сетки узлов, размещённых в многомерном пространстве. Изначально самоорганизующаяся карта представляет собой сетку из узлов, соединённую между собой связями. Кохонен рассматривал два варианта соединения узлов — в прямоугольную и гексагональную сетку — отличие состоит в том, что в прямоугольной сетке каждый узел соединён с четырьмя соседними, а в гексагональной — с шестью ближайшими узлами. Для двух таких сеток процесс построения сети Кохонена отличается лишь в том месте, где перебираются ближайшие к данному узлу соседи. Начальное вложение сетки в пространство данных выбирается произвольным образом. В авторском пакете SOM_PAK предлагаются варианты случайного начального расположения узлов в пространстве и вариант расположения узлов в плоскости. После этого узлы начинают перемещаться в пространстве согласно следующему алгоритму: Случайным образом выбирается точка данных x {\displaystyle x} . Определяется ближайший к x {\displaystyle x} узел карты (BMU — Best Matching Unit). Этот узел перемещается на заданный шаг по направлению к x {\displaystyle x} . Однако он перемещается не один, а увлекает за собой определённое количество ближайших узлов из некоторой окрестности на карте. Из всех двигающихся узлов наиболее сильно смещается центральный — ближайший к точке данных — узел, а остальные испытывают тем меньшие смещения, чем дальше они от BMU. В настройке карты различают два этапа — этап грубой ( ordering ) и этап тонкой ( fine-tuning ) настройки. На первом этапе выбираются большие значения окрестностей и движение узлов носит коллективный характер — в результате карта «расправляется» и грубым образом отражает структуру данных; на этапе тонкой настройки радиус окрестности равен 1-2 и настраиваются уже индивидуальные положения узлов. Кроме этого, величина смещения равномерно затухает со временем, то есть она велика в начале каждого из этапов обучения и близка к нулю в конце. Алгоритм повторяется определённое число эпох (понятно, что число шагов может сильно изменяться в зависимости от задачи). Известные типы сетей [ править | править код ] Перцептрон Розенблатта ; Сплайн-модель Хакимова ; Многослойный перцептрон Розенблатта ; Многослойный перцептрон Румельхарта ; Сеть Джордана ; Сеть Элмана ; Сеть Хэмминга ; Сеть Ворда ; Сеть Хопфилда ; Сеть Кохонена ; Нейронный газ [ 26 ] ; Когнитрон ; Неокогнитрон ; Хаотическая нейронная сеть ; Осцилляторная нейронная сеть ; Сеть встречного распространения ; Сеть радиально-базисных функций (RBF-сеть); Сеть обобщённой регрессии ; Сеть В. Чеботкова ; Вероятностная сеть ; Вероятностная нейронная сеть Решетова ; Сиамская нейронная сеть ; Сети адаптивного резонанса ; Свёрточная нейронная сеть ( англ. convolutional neural network ); Нечёткий многослойный перцептрон ; Импульсная нейронная сеть . Отличия от машин с архитектурой фон Неймана [ править | править код ] В разделе не хватает ссылок на источники (см. рекомендации по поиску ). Информация должна быть проверяема , иначе она может быть удалена. Вы можете отредактировать статью, добавив ссылки на авторитетные источники в виде сносок . ( 7 декабря 2023 ) Вычислительные системы , основанные на нейронных сетях, обладают рядом качеств, которые отсутствуют в машинах с архитектурой фон Неймана (но присущи мозгу человека): массовый параллелизм ; распределённое представление информации и вычисления ; способность к обучению и обобщению; адаптивность ; свойство контекстуальной обработки информации; толерантность к ошибкам; низкое энергопотребление. Примеры использований [ править | править код ] Предсказание финансовых временных рядов [ править | править код ] В разделе не хватает ссылок на источники (см. рекомендации по поиску ). Информация должна быть проверяема , иначе она может быть удалена. Вы можете отредактировать статью, добавив ссылки на авторитетные источники в виде сносок . ( 22 октября 2023 ) Входные данные — курс акций за год. Задача — определить завтрашний курс. Проводится следующее преобразование — выстраивается в ряд курс за сегодня, вчера, за позавчера. Следующий ряд — смещается по дате на один день и так далее. На полученном наборе обучается сеть с тремя входами и одним выходом — то есть выход: курс на дату, входы: курс на дату минус один день, минус два дня, минус три дня. Обученной сети подаётся на вход курс за сегодня, вчера, позавчера и получается ответ на завтра. В этом случае сеть выведет зависимость одного параметра от трёх предыдущих. Если желательно учитывать ещё какой-то параметр (например, общий индекс по отрасли), то его надо добавить как вход (и включить в примеры), переобучить сеть и получить новые результаты.
Для наиболее точного обучения стоит использовать метод ОРО , как наиболее предсказуемый и несложный в реализации. Психодиагностика [ править | править код ] Серия работ М. Г. Доррера с соавторами посвящена исследованию вопроса о возможности развития психологической интуиции у нейросетевых экспертных систем [ 27 ] [ 28 ] . Полученные результаты дают подход к раскрытию механизма интуиции нейронных сетей, проявляющейся при решении ими психодиагностических задач. Создан нестандартный для компьютерных методик интуитивный подход к психодиагностике , заключающийся в исключении построения описанной реальности . Он позволяет сократить и упростить работу над психодиагностическими методиками. Хемоинформатика [ править | править код ] Нейронные сети широко используются в химических и биохимических исследованиях [ 29 ] . В настоящее время нейронные сети являются одним из самых распространённых методов хемоинформатики для поиска количественных соотношений структура-свойство [ 30 ] [ 31 ] , благодаря чему они активно используются как для прогнозирования физико-химических свойств и биологической активности химических соединений, так и для направленного дизайна химических соединений и материалов с заранее заданными свойствами, в том числе при разработке новых лекарственных препаратов. Нейроуправление [ править | править код ] Основная статья: Нейроуправление Нейронные сети успешно применяются для синтеза систем управления динамическими объектами [ 32 ] [ 33 ] . В области управления нейронные системы находят применение в задачах идентификации объектов, в алгоритмах прогнозирования и диагностики, а также для синтеза оптимальных АСР. Для реализации АСР на основе ИНС в настоящее время интенсивно развивается производство нейрочипов и нейроконтроллеров (НК). В определённом смысле ИНС является имитатором мозга , обладающего способностью к обучению и ориентации в условиях неопределённости. Искусственная нейросеть сходна с мозгом в двух аспектах. Сеть приобретает знания в процессе обучения, а для сохранения знаний использует не сами объекты, а их связи — значения коэффициентов межнейронных связей, называемые синаптическими весами или синаптическими коэффициентами. Процедура обучения ИНС состоит в идентификации синаптических весов, обеспечивающих ей необходимые преобразующие свойства. Особенностью ИНС является её способность к модификации параметров и структуры в процессе обучения [ 34 ] . Экономика [ править | править код ] Алгоритмы нейронных сетей нашли широкое применение в экономике [ 35 ] . С помощью нейронных сетей решается задача разработки алгоритмов нахождения аналитического описания закономерностей функционирования экономических объектов (предприятие, отрасль, регион). Эти алгоритмы применяются к прогнозированию некоторых «выходных» показателей объектов. Применение нейросетевых методов позволяет решить некоторые проблемы экономико-статистического моделирования, повысить адекватность математических моделей, приблизить их к экономической реальности [ 36 ] . Поскольку экономические, финансовые и социальные системы очень сложны и являются результатом человеческих действий и противодействий, создание полной математической модели с учётом всех возможных действий и противодействий является очень сложной (если разрешимой) задачей. В системах подобной сложности естественным и наиболее эффективным является использование моделей, которые напрямую имитируют поведение общества и экономики. Именно это способна предложить методология нейронных сетей [ 37 ] . Самостоятельные системы [ править | править код ] Самостоятельные системы, такие как роботы, беспилотные автомобили и дроны, все чаще применяются в различных отраслях, а для их разработки нейросети играют ключевую роль в решении задач, таких как распознавание образов, анализ текста и управление системами. Нейросети используются для анализа данных, полученных от датчиков, для управления устройствами и принятия решений. Методы глубокого обучения часто применяются для решения проблем обнаружения и распознавания объектов, но возникают трудности при работе с объектами, находящимися в труднодоступных местах или в условиях плохой освещённости. В области автономного транспорта, нейросети являются примером заимствования концепций работы мозга и разума. Исследования используют нейронные сети для интеллектуального восприятия транспорта и определения типа транспорта [ источник не указан 530 дней ] [ 38 ] [ нет в источнике ] . Системы рекомендаций [ править | править код ] Нейросети представляют собой ценный инструмент для усовершенствования систем рекомендаций, которые основываются на нашей предыдущей активности, чтобы подсказывать нам что-то новое и интересное, например, музыку, фильмы или товары. Однако, с ростом объёма данных возникает потребность в более продвинутых методах обработки, и здесь на помощь приходят нейросети, которые способны обработать информацию, обнаружить скрытые связи между данными и определить, что может быть наиболее интересным и релевантным для нас. Например, они могут использоваться, чтобы с большей точностью рекомендовать товары, исходя из нашего прошлого поведения и покупок. Благодаря применению нейросетей в этой области, удаётся значительно улучшить качество рекомендаций и повысить уровень удовлетворения пользователей [ источник не указан 530 дней ] [ 39 ] [ нет в источнике ] . Нейросети в астрономии [ править | править код ] Нейросети в астрономии представляют собой мощный инструмент анализа данных, эволюционировавший через три исторические волны: от использования многослойных персептронов с ручным выбором параметров в 1980-х годах, к сверточным и рекуррентным нейронным сетям с прямой обработкой необработанных данных в 2010-х, и до современных генеративных и самообучающихся моделей без человеческого контроля. Сегодня, когда астрономические данные растут экспоненциально (удваиваясь каждые 16 месяцев), глубокое обучение становится необходимым для их обработки. В перспективе ожидается четвертая волна, когда астрономическое сообщество может перейти к использованию универсальных базовых моделей типа GPT, дообученных для астрономических задач [ 40 ] . См. также [ править | править код ] ChatGPT Оптические нейронные сети Искусственный интеллект Нейронный процессор Нейрокомпьютер Blue Brain Project Модель биологического нейрона Когнитивистика DeepMind Генеративный искусственный интеллект Примечания [ править | править код ] Комментарии [ править | править код ] ↑ По мнению известного специалиста по машинному обучению Ян Лекуна , машинное обучение есть воспроизведение мышления на основе искусственных нейронных сетей [ 4 ] Сноски [ править | править код ] ↑ Нейронные сети : [ арх. 25 октября 2022 ] / Галушкин А. И. // Большая российская энциклопедия : [в 35 т.] / гл. ред. Ю. С. Осипов . — М. : Большая российская энциклопедия, 2004—2017. ↑ 1 2 Мак-Каллок У. С. , Питтс В. Логическое исчисление идей, относящихся к нервной активности Архивная копия от 27 ноября 2007 на Wayback Machine // Автоматы / Под ред. К. Э. Шеннона и Дж. Маккарти. — М. : Изд-во иностр. лит., 1956. — С. 363—384. (Перевод английской статьи 1943 г.) ↑ Горбань А. Н. Кто мы, куда мы идём, как путь наш измерить? Архивная копия от 14 августа 2009 на Wayback Machine Пленарный доклад на открытии конференции Нейроинформатика-99 (МИФИ, 20 января 1999). Журнальный вариант: Горбань А. Н. Нейроинформатика: кто мы, куда мы идём, как путь наш измерить // Вычислительные технологии. — М. : Машиностроение. — 2000. — № 4. — С. 10—14. = Gorban A.N. Neuroinformatics: What are us, where are we going, how to measure our way? Архивная копия от 17 февраля 2016 на Wayback Machine The Lecture at the USA-NIS Neurocomputing Opportunities Workshop, Washington DC, July 1999 (Associated with IJCNN’99). ↑ Лекун, 2021 , с. 78. ↑ Н. Винер. Кибернетика. 2-е изд., 1961, гл. I. ↑ Голубев, 2007 , с. 4. ↑ Pattern Recognition and Adaptive Control. BERNARD WIDROW (неопр.) . Дата обращения: 9 февраля 2009. Архивировано 22 июня 2010 года. ↑ Уидроу Б., Стирнс С. Адаптивная обработка сигналов. — М. : Радио и связь, 1989. — 440 c. ↑ Петров А. П. О возможностях перцептрона // Известия АН СССР, Техническая кибернетика. — 1964. — № 6 . ↑ Бонгард М. М. Проблемы узнавания. — М. : Физматгиз, 1967. ↑ Голубев, 2007 , с. 5. ↑ Хакимов Б. Б. Моделирование корреляционных зависимостей сплайнами на примерах в геологии и экологии. — М. : Изд-во Моск. ун-та; СПб. : Нева, 2003. — 144 с. ↑ Werbos P. J. Beyond regression: New tools for prediction and analysis in the behavioral sciences. — Ph. D. thesis, Harvard University, Cambridge, MA, 1974. ↑ Галушкин А. И. Синтез многослойных систем распознавания образов. — М. : Энергия, 1974. ↑ Rumelhart D.E., Hinton G.E., Williams R.J. , Learning Internal Representations by Error Propagation. In: Parallel Distributed Processing, vol. 1, pp. 318—362. Cambridge, MA, MIT Press. 1986. ↑ Rumelhart D. E., Hinton G. E., Williams R. J. Learning Internal Representations by Error Propagation // Parallel Distributed Processing. Vol. 1. — Cambridge, MA: MIT Press, 1986. P. 318—362. ↑ Барцев С. И., Охонин В. А. Адаптивные сети обработки информации. — Красноярск: Ин-т физики СО АН СССР, 1986. Препринт № 59Б. — 20 с. ↑ Такой вид кодирования иногда называют кодом «1 из N» ↑ 1 2 Открытые системы — введение в нейросети Архивная копия от 31 октября 2005 на Wayback Machine ↑ Горбань А. Н. Обобщённая аппроксимационная теорема и вычислительные возможности нейронных сетей Архивная копия от 27 января 2012 на Wayback Machine // Сибирский журнал вычислительной математики , 1998, т. 1, № 1. — С. 12—24. ↑ Миркес Е. М. Логически прозрачные нейронные сети и производство явных знаний из данных Архивная копия от 4 февраля 2019 на Wayback Machine // Нейроинформатика / А. Н. Горбань, В. Л. Дунин-Барковский, А. Н. Кирдин и др. — Новосибирск: Наука. Сибирское предприятие РАН, 1998. — 296 с. — ISBN 5-02-031410-2 . ↑ Упоминание этой истории в журнале «Популярная механика» (неопр.) . Дата обращения: 16 апреля 2012. Архивировано 8 сентября 2011 года. ↑ INTUIT.ru — Рекуррентные сети как ассоциативные запоминающие устройства (неопр.) . Дата обращения: 5 октября 2007. Архивировано 17 декабря 2007 года. ↑ Kohonen T. Self-Organizing Maps. 3rd edition. — Berlin — New York: Springer-Verlag, 2001/ — ISBN 0-387-51387-6 , ISBN 3-540-67921-9 . ↑ Зиновьев А. Ю. Визуализация многомерных данных . — Красноярск: Изд-во Красноярского гос. техн. ун-та, 2000. — 180 с. Архивировано 6 марта 2019 года. ↑ Martinetz T. M., Berkovich S. G., Schulten K. J. Neural-gas network for vector quantization and its application to time-series prediction Архивная копия от 16 июля 2019 на Wayback Machine // IEEE Trans. on Neural Networks, 1993, No. 4. — P. 558—569. На сайте PCA Архивная копия от 16 марта 2019 на Wayback Machine ↑ Gorban A. N., Rossiyev D. A., Dorrer M. G. MultiNeuron — Neural Networks Simulator For Medical, Physiological, and Psychological Applications Архивная копия от 17 февраля 2016 на Wayback Machine // Wcnn’95, Washington, D.C.: World Congress on Neural Networks, 1995. International Neural Network Society Annual Meeting : Renaissance Hotel, Washington, D.C., USA, July 17-21, 1995. ↑ Доррер М. Г. Психологическая интуиция искусственных нейронных сетей Архивная копия от 25 марта 2009 на Wayback Machine , Дисс. … 1998. Другие копии онлайн: [1] Архивная копия от 28 апреля 2009 на Wayback Machine , [2] Архивная копия от 7 апреля 2009 на Wayback Machine ↑ Баскин И. И., Палюлин В. А., Зефиров Н. С. Применение искусственных нейронных сетей в химических и биохимических исследованиях Архивная копия от 10 июля 2007 на Wayback Machine // Вестн. Моск. ун-та. Сер. 2. Химия . 1999. Т. 40. № 5. ↑ Гальберштам Н. М., Баскин И. И., Палюлин В. А., Зефиров Н. С. Нейронные сети как метод поиска зависимостей структура — свойство органических соединений (рус.) // Успехи химии . — Российская академия наук , 2003. — Т. 72 , № 7 . — С. 706—727 . Архивировано 29 марта 2023 года. ↑ Баскин И. И., Палюлин В. А., Зефиров Н. С. Многослойные персептроны в исследовании зависимостей «структура-свойство» для органических соединений // Российский химический журнал (Журнал Российского химического общества им. Д. И. Менделеева). — 2006. — Т. 50 . — С. 86—96 . ↑ Сигеру, Марзуки, Рубия, 2000 . ↑ Чернодуб А. Н., Дзюба Д. А. Обзор методов нейроуправления Архивная копия от 13 января 2012 на Wayback Machine // Проблемы программирования . — 2011. — № 2. — С. 79—94. ↑ Сабании В. Р. Автоматические системы регулирования на основе нейросетевых технологий / В. Р. Сабанин, Н. И. Смирнов, А. И. Репин // Труды Международной научной конференции Control-2003. М.: Издательство МЭИ, 2003. С. 45—51. ↑ Калацкая Л. В., Новиков В. А., Садов В. С. Организация и обучение искусственных нейронных сетей: Экспериментальное учеб. пособие. — Минск: Изд-во БГУ, 2003. — 72 с. ↑ Кенин А. М., Мазуров В. Д. Опыт применения нейронных сетей в экономических задачах Архивная копия от 2 апреля 2013 на Wayback Machine ↑ [3] Нейронные сети в экономике ↑ Виталий Валентинович Гаевский, Андрей Михайлович Иванов. ПРОБЛЕМЫ ПРИМЕНЕНИЯ ИНТЕЛЛЕКТУАЛЬНЫХ СИСТЕМ ПОМОЩИ ВОДИТЕЛЮ НА ОДНОКОЛЕЙНЫХ ТРАНСПОРТНЫХ СРЕДСТВАХ // Transactions of NNSTU n.a. R.E. Alekseev. — 2018. — Вып. 3 . — С. 121–129 . — ISSN 1816-210X . — doi : 10.46960/1816-210x_2018_3_121 . ↑ Ризван Рамзанович Турлуев. НЕЙРОСЕТИ В СИСТЕМАХ КОРПОРАТИВНОГО УПРАВЛЕНИЯ // НЕЙРОСЕТИ В СИСТЕМАХ КОРПОРАТИВНОГО УПРАВЛЕНИЯ. — ICSP «NEW SCIENCE», 2021. ↑ Michael J. Smith, James E. Geach. Astronomia ex machina: a history, primer and outlook on neural networks in astronomy // Royal Society Open Science. — 2023-05-31. — Т. 10 , вып. 5 . — С. 221454 . — doi : 10.1098/rsos.221454 . Литература [ править | править код ] Беркинблит М. Б. Нейронные сети. — М. : МИРОС и ВЗМШ РАО, 1993. — 96 с. — ISBN 5-7084-0026-9 . Вороновский Г. К., Махотило К. В., Петрашев С. Н., Сергеев С. А. Генетические алгоритмы, искусственные нейронные сети и проблемы виртуальной реальности. — Харьков: Основа, 1997. — 112 с. — ISBN 5-7768-0293-8 . Голубев Ю. Ф. Нейросетевые методы в мехатронике. — М. : Изд-во Моск. унта, 2007. — 157 с. — ISBN 978-5-211-05434-9 . Горбань А. Н. Обучение нейронных сетей . — М. : СССР-США СП «Параграф», 1990. — 160 с. Горбань А. Н., Россиев Д. А. Нейронные сети на персональном компьютере . — Новосибирск: Наука, 1996. — 276 с. — ISBN 5-02-031196-0 . Горбань А. Н., Дунин-Барковский В. Л. и др. Нейроинформатика . — Новосибирск: Наука, 1998. Гудфеллоу Я., Бенджио И., Курвилль А. Глубокое обучение = Deep Learning. — М. : ДМК-Пресс , 2017. — 652 с. — ISBN 978-5-97060-554-7 . Ерёмин Д. М., Гарцеев И. Б. Искусственные нейронные сети в интеллектуальных системах управления. — М. : МИРЭА, 2004. — 75 с. — ISBN 5-7339-0423-2 . Каллан Р. Основные концепции нейронных сетей = The Essence of Neural Networks First Edition. — М. : Вильямс, 2001. — 288 с. — ISBN 5-8459-0210-X . Круглов В. В. , Борисов В. В. Искусственные нейронные сети. Теория и практика. — М. : Горячая линия - Телеком, 2001. — 382 с. — ISBN 5-93517-031-0 . Миркес Е. М. Нейрокомпьютер. Проект стандарта . — Новосибирск: Наука, 1999. — 337 с. — ISBN 5-02-031409-9 . Другие копии онлайн: Нейрокомпьютер. Проект стандарта . Николенко С. , Кадурин А., Архангельская Е. Глубокое обучение. — СПб. : Питер , 2018. — 480 с. — ISBN 978-5-496-02536-2 . Осовский Станислав. Нейронные сети для обработки информации = Sieci neuronowe do przetwarzania informacji (пол.) / Перевод И. Д. Рудинского. — М. : Финансы и статистика, 2004. — 344 с. — ISBN 5-279-02567-4 . Савельев А. В. На пути к общей теории нейросетей. К вопросу о сложности // Нейрокомпьютеры: разработка, применение. — 2006. — № 4—5 . — С. 4—14 . Архивировано 11 сентября 2016 года. Сигеру Омату, Марзуки Халид, Рубия Юсоф. Нейроуправление и его приложения = Neuro-Control and its Applications. 2-е изд. — М. : ИПРЖР, 2000. — 272 с. — ISBN 5-93108-006-6 . Тадеусевич Рышард, Боровик Барбара, Гончаж Томаш, Леппер Бартош. Элементарное введение в технологию нейронных сетей с примерами программ / Перевод И. Д. Рудинского. — М. : Горячая линия — Телеком, 2011. — 408 с. — ISBN 978-5-9912-0163-6 . . Терехов В. А., Ефимов Д. В., Тюкин И. Ю. Нейросетевые системы управления. — М. : Высшая школа , 2002. — 184 с. — ISBN 5-06-004094-1 . Уоссермен Ф. Нейрокомпьютерная техника: Теория и практика = Neural Computing. Theory and Practice. — М. : Мир, 1992. — 240 с. — ISBN 5-03-002115-9 . Архивная копия от 30 июня 2009 на Wayback Machine Хайкин С. Нейронные сети: полный курс = Neural Networks: A Comprehensive Foundation. 2-е изд. — М. : Вильямс, 2006. — 1104 с. — ISBN 0-13-273350-1 . Ясницкий Л. Н. Введение в искусственный интеллект. — М. : Издат. центр «Академия», 2005. — 176 с. — ISBN 5-7695-1958-4 . Ян Лекун . Как учится машина. Революция в области нейронных сетей и глубокого обучения. (Библиотека Сбера: Искусственный интеллект). — М. : Альпина нон-фикшн, 2021. — 348 с. — ISBN 978-5-907394-29-2 . Ссылки [ править | править код ] Neural Networks в каталоге ссылок Curlie (dmoz) Учебник по искусственным нейронным сетям Ian Goodfellow, Yoshua Bengio, Aaron Courville. Deep Learning (англ.) . MIT Press (2016). — Книга по машинному обучению и, в частности, по глубокому обучению . Ссылки на внешние ресурсы Словари и энциклопедии Большая китайская Большая китайская Большая китайская Большая китайская Большая норвежская Большая российская (научно-образовательный портал) PWN Treccani Universalis В библиографических каталогах GND : 4226127-2 J9U : 987007551192405171 LCCN : sh90001937 LNB : 000053170 NDL : 01165604 NKC : ph115443 Типы искусственных нейронных сетей Сеть прямого распространения Сеть радиально-базисных функций Однослойный перцептрон Многослойный перцептрон Розенблата Румельхарта Сеть Хопфилда Цепь Маркова Машина Больцмана Ограниченная машина Больцмана Автокодировщик Шумоподавляющий автокодировщик [англ.] Разреженный автокодировщик [англ.] Вариационный автокодировщик [англ.] Глубокая сеть доверия Свёрточная нейронная сеть Глубинная свёрточная нейронная сеть Развёртывающая нейронная сеть Глубинная свёрточная обратная графическая сеть Генеративно-состязательная сеть Рекуррентная нейронная сеть Рекурсивные нейронные сети Долгая краткосрочная память Управляемый рекуррентный блок Нейронные машины Тьюринга [англ.] Двунаправленная сеть Двунаправленная рекуррентная нейросеть [англ.] Двунаправленная сеть с долгой краткосрочной памятью Двунаправленные управляемые рекуррентные нейроны Глубинная остаточная сеть Нейронная эхо-сеть [англ.] Метод экстремального обучения [англ.] Метод неустойчивых состояний [англ.] Метод опорных векторов Сеть Кохонена Самоорганизующаяся карта Кохонена Капсульная нейронная сеть Ассоциативная память на нейронных сетях Искусственный интеллект История Вычислительные машины и разум Зима искусственного интеллекта Бум искусственного интеллекта Джорджтаунский эксперимент Дартмутский семинар Отчёт Лайтхилла Регламент ЕС Гонка вооружений в области искусственного интеллекта Холодная война за искусственный интеллект Философия Тест Тьюринга Китайская комната Сильный и слабый искусственные интеллекты Дружественный искусственный интеллект Этика искусственного интеллекта Проблема контроля Направления Агентный подход Адаптивное управление Генеративный ИИ Инженерия знаний Модель жизнеспособной системы Машинное обучение Нейронная сеть Нечёткая логика Обработка естественного языка Персональный искусственный интеллект Распознавание образов Роевой интеллект Символический ИИ Эволюционные алгоритмы Экспертная система Применение Голосовое управление Задача классификации Классификация документов Кластеризация документов Кластерный анализ Локальный поиск Машинный перевод Оптическое распознавание символов Распознавание речи Распознавание рукописного ввода Игровой ИИ Исследователи Чарлз Бэббидж Владимир Вапник Джозеф Вейценбаум Норберт Винер Виктор Глушков Владимир Городецкий Рэймонд Курцвейл Ян Лекун Алексей Ляпунов Джон Маккарти Марвин Мински Аллен Ньюэлл Сеймур Пейперт Джуда Перл Гермоген Поспелов Дмитрий Поспелов Фрэнк Розенблатт Герберт Саймон Алан Тьюринг Патрик Уинстон Виктор Финн Сергей Фомин Демис Хассабис Джеффри Хинтон Ноам Хомский Клод Шеннон Эндрю Ын Элиезер Юдковский Инженерия знаний Общие понятия Данные Метаданные Знания Метазнания Представление знаний База знаний Онтология Семантическая паутина Жёсткие модели Продукции Семантические сети Фреймы Логическая модель Мягкие методы Нейронная сеть Эволюционное моделирование Нечёткая логика Применения Экспертные системы Интеллектуальный анализ данных Извлечение информации Виртуальные собеседники Гибридные интеллектуальные системы Искусственный интеллект Машинное обучение Обработка естественного языка Машинное обучение и data mining Задачи Задача классификации Обучение без учителя Обучение с частичным привлечением учителя Регрессионный анализ AutoML Ассоциативные правила Выделение признаков Обучение признакам Обучение ранжированию Грамматический вывод Онлайновое обучение Обучение с учителем Метод k ближайших соседей Наивный байесовский классификатор Дерево решений Метод опорных векторов Линейная регрессия Логистическая регрессия Перцептрон Ансамблевое обучение Бэггинг Бустинг Метод случайного леса Метод релевантных векторов Кластерный анализ Метод k-средних Метод нечёткой кластеризации Иерархическая кластеризация EM-алгоритм BIRCH CURE DBSCAN OPTICS Mean-shift Снижение размерности Факторный анализ Метод главных компонент CCA ICA LDA Неотрицательное матричное разложение t-SNE Структурное прогнозирование Графовая вероятностная модель Байесовская сеть Скрытая марковская модель CRF Выявление аномалий Метод k ближайших соседей Локальный уровень выброса Графовые вероятностные модели Байесовская сеть Марковская сеть Скрытая марковская модель Нейронные сети Ограниченная машина Больцмана Самоорганизующаяся карта Функция активации Сигмоида Softmax Радиально-базисная функция Метод обратного распространения ошибки Глубокое обучение Многослойный перцептрон Рекуррентная нейронная сеть Долгая краткосрочная память Управляемый рекуррентный блок Свёрточная нейронная сеть U-Net Автокодировщик Обучение с подкреплением Марковский процесс Уравнение Беллмана Жадный алгоритм Q-обучение SARSA Temporal difference (TD) Теория Размерность Вапника — Червоненкиса Дилемма смещения–дисперсии Теория вычислительного обучения Минимизация эмпирического риска Оккамово обучение PAC learning Статистическая теория обучения Журналы и конференции NeurIPS ICML ML JMLR ArXiv:cs.LG Генеративный ИИ Концепции Автокодировщик Глубокое обучение Генеративно-состязательная сеть Generative pre-trained transformer Большая языковая модель Нейронная сеть Техника подсказок RAG [укр.] RLHF Самоуправляемое обучение [укр.] Трансформер Вариационный автокодировщик [укр.] Зрительный трансформер [укр.] Векторное представление слов Модели Текст BLOOM Claude DBRX DeepSeek Gemini GigaChat GPT 1 [англ.] 2 [англ.] 3 J ChatGPT 4 4o [англ.] 4.5 [англ.] OpenAI o1 OpenAI o3 Grok Granite Qwen LaMDA LLaMA Mistral Large [англ.] PanGu-Σ [англ.] YandexGPT Perplexity AI Программный код GigaCode GitHub Copilot Granite.Code OpenAI Codex Изображение Aurora DALL-E Firefly Flux Ideogram Kandinsky Midjourney Stable Diffusion Шедеврум Видео Dream Machine [англ.] Gen-3 Alpha [англ.] Hailuo AI Kling [англ.] Sora Veo Музыка Udio Suno AI Компании Alibaba Anthropic Baichuan ElevenLabs [англ.] Google DeepMind Hugging Face Kuaishou [англ.] Meta AI MiniMax [англ.] Mistral AI [англ.] Moonshot AI [англ.] OpenAI Runway [англ.] Sber AI Stability AI The Stargate Project Synthesia [англ.] xAI Zhipu AI Яндекс Категория · Искусственный интеллект Источник — https://ru.wikipedia.org/w/index.php?title=Нейронная_сеть&oldid=144273680 Категория : Искусственные нейронные сети Скрытые категории: Википедия:Cite web (не указан язык) Страницы, использующие волшебные ссылки ISBN Википедия:Статьи с разделами без ссылок на источники с декабря 2023 года Википедия:Статьи без источников (тип: специальность) Википедия:Статьи с разделами без ссылок на источники с ноября 2023 года Википедия:Статьи, требующие уточнения источников Википедия:Статьи с шаблонами недостатков по алфавиту Википедия:Нет источников с июня 2013 Википедия:Статьи с утверждениями без источников более 14 дней Википедия:Нет источников с декабря 2017 Википедия:Статьи с разделами без ссылок на источники с октября 2023 года Википедия:Нет источников с октября 2023 Википедия:Статьи с утверждениями, не найденными в указанном источнике Википедия:Нет в источнике с октября 2023 Навигация Поиск